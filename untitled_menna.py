# -*- coding: utf-8 -*-
"""Untitled_menna.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KhtVG-HKwL1gs6Aelj5w3Fkgd4B3jNkf
"""

!pip install langchain

pip install --upgrade langchain langchain-community transformers

import random
import json

# Categories and attributes for scaling the catalog
categories = ["Electronics", "Furniture", "Stationery", "Clothing", "Accessories", "Home Appliances", "laptop"]
delivery_times = ["Same-day", "Next-day", "2-3 days", "1 week"]

# Generate product catalog
def generate_product_catalog(num_items):
    catalog = []
    for i in range(1, num_items + 1):
        product = {
            "id": i,
            "name": f"Product {i}",
            "price": round(random.uniform(5, 2000), 2),
            "category": random.choice(categories),
            "stock": random.randint(0, 200),
            "delivery_time": random.choice(delivery_times),
        }
        catalog.append(product)
    return catalog

PRODUCT_catalog = generate_product_catalog(1000)

# Save the catalog to a JSON file for reuse
# âœ… Specify file path (change path as needed)
file_path = "PRODUCT_catalog"
# âœ… Save to JSON file
with open(file_path, "w") as f:
    json.dump(PRODUCT_catalog, f, indent=4)

print(f"JSON file saved as: {file_path}")

import pandas as pd

# Convert JSON to DataFrame for fast filtering
df_catalog = pd.read_json("/content/PRODUCT_catalog")
df_catalog.head()

import torch
import os
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
#from langchain.llms import HuggingFacePipeline
from langchain_community.llms import HuggingFacePipeline



# âœ… Load Product Catalog
json_path = "/content/PRODUCT_catalog"
if not os.path.exists(json_path):
    raise FileNotFoundError(f"ERROR: The JSON file '{json_path}' is missing.")
df_catalog = pd.read_json(json_path)

#The Language Model:

#from langchain_community import HuggingFacePipeline
from langchain.chains import LLMChain # Import LLMChain
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM, PhiForCausalLM
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
if device != 'cuda':
    print('Sorry no cuda.')

#model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM
#tokenizer = AutoTokenizer.from_pretrained(model_id)
#model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False)

# Load model directly
# Use a pipeline as a high-level helper
lang_model = PhiForCausalLM.from_pretrained("microsoft/phi-1")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1")
prompt = "This is an example script ."
inputs = tokenizer(prompt, return_tensors="pt")

# Generate
generate_ids = lang_model.generate(inputs.input_ids, max_new_tokens=20, temperature=0.2)
tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]

pipe = pipeline("text-generation", model="microsoft/phi-1_5", max_new_tokens=300, temperature=0.2)

local_llm = HuggingFacePipeline(pipeline=pipe)

# âœ… Store Chat History for Context
def chat_with_bot(query):
    global chat_history, df_catalog

    # âœ… Step 1: Retrieve relevant information from JSON
    search_results = df_catalog[df_catalog.apply(lambda row: query.lower() in str(row).lower(), axis=1)]

    if search_results.empty:
        extracted_info = "I'm sorry, I couldn't find relevant information in the product catalog."
    else:
        extracted_info = search_results.to_string(index=False)  # Convert relevant data to string

    # âœ… Step 2: Format input for LLM
    formatted_history = " ".join(chat_history[-5:])  # Keep last 5 messages
    input_text = f"{formatted_history} User: {query} \n Product Info: {extracted_info} \n AI:"

    # âœ… Step 3: Generate response using LLM
    response = local_llm(input_text)
    generated_text = response[0]["generated_text"].split("AI:")[-1].strip()

    # âœ… Step 4: Store conversation
    chat_history.append(f"User: {query}")
    chat_history.append(f"AI: {generated_text}")

    return generated_text


# âœ… Example Queries
queries = [
    "Hello, what can you help me with?",
    "Show me Apple laptops under $1000",
    "I need the best electronics",
    "Can you compare laptops?",
    "Which products are in stock?",
    "Find me a smartphone under $500",
]

for query in queries:
    print(f"Query: {query}")
    print(chat_with_bot(query))
    print("-" * 50)

!pip install -U langchain-huggingface

"""working but not from JSON file"""

import torch
import os
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline  # âœ… Updated import
from langchain.chains import LLMChain

# âœ… Load Product Catalog
json_path = "/content/PRODUCT_catalog"  # Added .json extension
if not os.path.exists(json_path):
    raise FileNotFoundError(f"ERROR: The JSON file '{json_path}' is missing.")
df_catalog = pd.read_json(json_path)

# âœ… Set up device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
if device != 'cuda':
    print('Sorry no cuda.')

# âœ… Load Hugging Face Model (Phi-1.5)
model_id = "microsoft/phi-1_5"
tokenizer = AutoTokenizer.from_pretrained(model_id)
lang_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

# âœ… Define Text Generation Pipeline
pipe = pipeline("text-generation", model=lang_model, tokenizer=tokenizer, max_new_tokens=300, temperature=0.2, do_sample=True)  # Fixed `do_sample=True`

# âœ… Initialize LLM Wrapper
local_llm = HuggingFacePipeline(pipeline=pipe)

# âœ… Initialize chat history
chat_history = []

# âœ… Store Chat History for Context
def chat_with_bot(query):
    global chat_history, df_catalog

    # âœ… Step 1: Retrieve relevant information from JSON
    search_results = df_catalog[df_catalog.apply(lambda row: query.lower() in str(row).lower(), axis=1)]

    if search_results.empty:
        extracted_info = "I'm sorry, I couldn't find relevant information in the product catalog."
    else:
        extracted_info = search_results.to_string(index=False)  # Convert relevant data to string

    # âœ… Step 2: Format input for LLM
    formatted_history = " ".join(chat_history[-5:])  # Keep last 5 messages
    input_text = f"{formatted_history} User: {query} \n Product Info: {extracted_info} \n AI:"

    # âœ… Step 3: Generate response using LLM
    response = local_llm(input_text)

    if isinstance(response, str):  # Fix potential list error
        generated_text = response.strip()
    else:
        generated_text = response[0]["generated_text"].split("AI:")[-1].strip()

    # âœ… Step 4: Store conversation
    chat_history.append(f"User: {query}")
    chat_history.append(f"AI: {generated_text}")

    return generated_text


# âœ… Example Queries
queries = [
    "Hello, what can you help me with?",
    "Show me Apple laptops under $1000",
    "I need the best electronics",
    "Can you compare laptops?",
    "Which products are in stock?",
    "Find me a smartphone under $500",
]

for query in queries:
    print(f"Query: {query}")
    print(chat_with_bot(query))
    print("-" * 50)

"""TRIAL FOR STREAMLIT"""

pip install streamlit

import torch
import os
import pandas as pd
import streamlit as st  # âœ… Import Streamlit
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline  # âœ… Updated import

# âœ… Streamlit App Title
st.title("ðŸ›’ AI-Powered E-Commerce Chatbot")

# âœ… Load Product Catalog
json_path = "/content/PRODUCT_catalog"  # Ensure the correct file path
if not os.path.exists(json_path):
    st.error(f"ERROR: The JSON file '{json_path}' is missing.")
    st.stop()
df_catalog = pd.read_json(json_path)

# âœ… Set up device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
if device != 'cuda':
    st.warning('CUDA is not available. Running on CPU.')

# âœ… Load Hugging Face Model (Phi-1.5)
model_id = "microsoft/phi-1_5"
tokenizer = AutoTokenizer.from_pretrained(model_id)
lang_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)

# âœ… Define Text Generation Pipeline
pipe = pipeline("text-generation", model=lang_model, tokenizer=tokenizer, max_new_tokens=300, temperature=0.2, do_sample=True)

# âœ… Initialize LLM Wrapper
local_llm = HuggingFacePipeline(pipeline=pipe)

# âœ… Streamlit Session State for Chat History
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# âœ… Chatbot Function
def chat_with_bot(query):
    global df_catalog

    # âœ… Step 1: Retrieve relevant information from JSON
    search_results = df_catalog[df_catalog.apply(lambda row: query.lower() in str(row).lower(), axis=1)]

    if search_results.empty:
        extracted_info = "I'm sorry, I couldn't find relevant information in the product catalog."
    else:
        extracted_info = search_results.to_string(index=False)  # Convert relevant data to string

    # âœ… Step 2: Format input for LLM
    formatted_history = " ".join(st.session_state.chat_history[-5:])  # Keep last 5 messages
    input_text = f"{formatted_history} User: {query} \n Product Info: {extracted_info} \n AI:"

    # âœ… Step 3: Generate response using LLM
    response = local_llm(input_text)

    if isinstance(response, str):  # Fix potential list error
        generated_text = response.strip()
    else:
        generated_text = response[0]["generated_text"].split("AI:")[-1].strip()

    # âœ… Step 4: Store conversation
    st.session_state.chat_history.append(f"User: {query}")
    st.session_state.chat_history.append(f"AI: {generated_text}")

    return generated_text

# âœ… Streamlit Chat Interface
st.subheader("Chat with your AI Assistant")
user_query = st.text_input("Ask me anything about our products:")

if st.button("Send"):
    if user_query:
        response = chat_with_bot(user_query)
        st.write(f"**AI:** {response}")

# âœ… Display Chat History
st.subheader("Chat History")
for msg in st.session_state.chat_history:
    st.write(msg)